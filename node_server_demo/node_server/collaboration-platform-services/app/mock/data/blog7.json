{
  "blog_id": "blog_mock_7",
  "title": "Mystery Behind Execution Of Spark Jobs",
  "author": null,
  "content": "<p><span style=\"font-family: georgia, palatino; font-size: 12pt;\">Ever attempted to write a Spark Job and wondered what was happening ? We will figure out what is happening to the Spark Jobs.</span> <strong><span style=\"text-decoration: underline; font-size: 14pt; font-family: georgia, palatino;\">Spark Application</span></strong> <span style=\"font-family: georgia, palatino; font-size: 12pt;\">A Spark Application is essentially composed of a driver program and a set of executers. Spark applications can be considered similar to map reduce jobs. Spark introduces the concept of Resilient Distributed Datasets(RDD's). RDD is fault tolerant collection of elements that can be worked upon in parallel. The executers spans across nodes in the cluster.</span> <a href=\"https://vista-mindtree.cloudapp.net/coe_open/wp-content/uploads/2015/09/sparkex.png\"><img class=\"aligncenter  wp-image-5317\" src=\"https://vista-mindtree.cloudapp.net/coe_open/wp-content/uploads/2015/09/sparkex.png\" alt=\"sparkex\" width=\"579\" height=\"337\" /></a> <span style=\"font-family: georgia, palatino; font-size: 12pt;\"><strong><span style=\"font-size: 14pt;\">Step 1:</span></strong> Spark Context talks to the cluster manager  and aquires the executor.  Driver - The main program that we have written which has the spark context is the driver .</span> <span style=\"font-family: georgia, palatino; font-size: 12pt;\"><span style=\"font-size: 14pt;\"><strong>Step 2:</strong></span> Executor - consists of processes that runs computations and stores the application data. The Application code is passed onto the executors.</span> <span style=\"font-family: georgia, palatino; font-size: 12pt;\"><strong><span style=\"font-size: 14pt;\">Step 3:</span></strong> Spark context send tasks to the executor to run.</span> <span style=\"font-size: 12pt;\"><a href=\"https://vista-mindtree.cloudapp.net/coe_open/wp-content/uploads/2015/09/sparkjob.png\"><img class=\"alignnone  wp-image-5307\" src=\"https://vista-mindtree.cloudapp.net/coe_open/wp-content/uploads/2015/09/sparkjob.png\" alt=\"sparkjob\" width=\"595\" height=\"489\" /></a></span> <span style=\"font-size: 12pt; font-family: georgia, palatino;\">The application gets its own executor processes and it runs tasks in multiple threads . Data can be shared between spark applications by writing  onto external storage.</span> <strong><span style=\"font-size: 14pt; font-family: georgia, palatino;\"> What happens when I invoke an action in the Spark Application ?</span></strong> <span style=\"font-family: georgia, palatino; font-size: 12pt;\">Job - Spark Job is launched to full fill it. Spark comes up with an execution plan.(By examining the RDDS). An execution plan consists of assembling the jobs transformation into stages.</span> <strong><span style=\"font-size: 14pt; font-family: georgia, palatino;\">Stage:</span></strong> <span style=\"font-size: 12pt; font-family: georgia, palatino;\"> Is a collection of tasks all of which execute the  same code on different data sets.</span> <span style=\"font-size: 12pt; font-family: georgia, palatino;\"> Each stage consists of sequence of transfomations that can be executed without the shuffling of data.</span> <strong><span style=\"font-size: 14pt; font-family: georgia, palatino;\">Resilient Distributed Data Sets (RDD)</span></strong> <span style=\"font-family: georgia, palatino; font-size: 12pt;\">Resilient distributed dataset is a fault-tolerant collection of elements that can be operated on in parallel. An RDD consists of fixed number of partitions.</span> <span style=\"font-size: 12pt;\"><a href=\"https://vista-mindtree.cloudapp.net/coe_open/wp-content/uploads/2015/09/rdd.png\"><img class=\"alignnone size-medium wp-image-5310\" src=\"https://vista-mindtree.cloudapp.net/coe_open/wp-content/uploads/2015/09/rdd-300x181.png\" alt=\"rdd\" width=\"300\" height=\"181\" /></a></span> <strong><span style=\"font-size: 14pt; font-family: georgia, palatino;\"> Partitions </span></strong> <span style=\"font-size: 12pt;\"><span style=\"font-family: georgia, palatino;\">Consisits of number of records.Map and Filter are called narrow transformations .T</span></span><span style=\"font-family: georgia, palatino;\"><span style=\"font-size: 12pt;\">he RDDs returned by narrow transformations like map and filter, the records required </span><span style=\"font-size: 12pt;\">to compute the records in a single partition reside in a single partition in the parent RDD. </span></span></p>\n",
  "status": "PUBLISHED",
  "viewed_by": [],
  "tags": ["spark", "bigdata", "analytics"],
  "liked_by": [],
  "comments": [],
  "dateOfPublishing": "2016-06-17T16:25:24.306Z"
}